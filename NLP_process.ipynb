{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsM7itbrgYs3YRLjS8eyj0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SriRamK345/NLP_dataset/blob/main/NLP_process.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is NLP?\n",
        "NLP stands for Natural Language Processing, which is a part of Computer Science, Human language, and Artificial Intelligence. It is the technology that is used by machines to understand, analyse, manipulate, and interpret human's languages. It helps developers to organize knowledge for performing tasks such as translation, automatic summarization, Named Entity Recognition (NER), speech recognition, relationship extraction, and topic segmentation.\n",
        "\n",
        "https://www.javatpoint.com/nlp#What\n",
        "\n",
        "https://www.geeksforgeeks.org/natural-language-processing-nlp-pipeline/?ref=ml_lbp"
      ],
      "metadata": {
        "id": "V1CgZLH_XQXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Text Processing and Preprocessing In NLP\n",
        "- **Tokenization:** Dividing text into smaller units, such as words or sentences.\n",
        "- **Stemming and Lemmatization:** Reducing words to their base or root forms.\n",
        "- **Stopword Removal:** Removing common words (like “and”, “the”, “is”) that may not carry significant meaning.\n",
        "- **Text Normalization:** Standardizing text, including case normalization, removing punctuation, and correcting spelling errors."
      ],
      "metadata": {
        "id": "r4LGJTYsaZ5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Components of NLP**\n",
        "There are the following two components of NLP -\n",
        "\n",
        "**1. Natural Language Understanding (NLU)**\n",
        "\n",
        "Natural Language Understanding (NLU) helps the machine to understand and analyse human language by extracting the metadata from content such as concepts, entities, keywords, emotion, relations, and semantic roles.\n",
        "\n",
        "**2. Natural Language Generation (NLG)**\n",
        "\n",
        "Natural Language Generation (NLG) acts as a translator that converts the computerized data into natural language representation. It mainly involves Text planning, Sentence planning, and Text Realization.\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "p6wdrMQTboVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step-by-Step NLP Pipeline with Code Examples**\n",
        "\n",
        "#### **1. Import Libraries**\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.tree import Tree\n",
        "import spacy\n",
        "import re\n",
        "```\n",
        "\n",
        "**Install Required Libraries:**\n",
        "\n",
        "```bash\n",
        "pip install nltk spacy\n",
        "python -m spacy download en_core_web_sm\n",
        "```\n"
      ],
      "metadata": {
        "id": "DmgbSN2cboaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.tree import Tree\n",
        "import spacy\n",
        "import re"
      ],
      "metadata": {
        "id": "RRFGVgk7ov9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. Sentence Segmentation**\n",
        "\n",
        "Step1: Sentence Segmentation\n",
        "\n",
        "Sentence Segment is the first step for building the NLP pipeline. It breaks the paragraph into separate sentences.\n",
        "\n",
        "Example: Consider the following paragraph -\n",
        "\n",
        "Independence Day is one of the important festivals for every Indian citizen. It is celebrated on the 15th of August each year ever since India got independence from the British rule. The day celebrates independence in the true sense.\n",
        "\n",
        "Sentence Segment produces the following result:\n",
        "\n",
        "\"Independence Day is one of the important festivals for every Indian citizen.\"\n",
        "\"It is celebrated on the 15th of August each year ever since India got independence from the British rule.\"\n",
        "\"This day celebrates independence in the true sense.\"\n",
        "\n",
        "**Code Example:**"
      ],
      "metadata": {
        "id": "jyWRn9_mgjId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Example paragraph\n",
        "paragraph = \"\"\"Independence Day is one of the important festivals for every Indian citizen.\n",
        "It is celebrated on the 15th of August each year ever since India got independence from the British rule.\n",
        "The day celebrates independence in the true sense.\"\"\"\n",
        "\n",
        "# Sentence Segmentation\n",
        "sentences = sent_tokenize(paragraph)\n",
        "print(\"Sentences:\\n\", sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "483Ng-ugfhGs",
        "outputId": "46b9681a-b7e7-4256-adf9-41a1bbda7c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences:\n",
            " ['Independence Day is one of the important festivals for every Indian citizen.', 'It is celebrated on the 15th of August each year ever since India got independence from the British rule.', 'The day celebrates independence in the true sense.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. Word Tokenization**\n",
        "\n",
        "Word Tokenizer is used to break the sentence into separate words or tokens.\n",
        "\n",
        "Example:\n",
        "\n",
        "JavaTpoint offers Corporate Training, Summer Training, Online Training, and Winter Training.\n",
        "\n",
        "Word Tokenizer generates the following result:\n",
        "\n",
        "\"JavaTpoint\", \"offers\", \"Corporate\", \"Training\", \"Summer\", \"Training\", \"Online\", \"Training\", \"and\", \"Winter\", \"Training\", \".\"\n",
        "**Code Example:**"
      ],
      "metadata": {
        "id": "Njv2cWBhgITT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Example sentence\n",
        "sentence = sentences[0]\n",
        "\n",
        "# Word Tokenization\n",
        "words = word_tokenize(sentence)\n",
        "print(\"Words:\", words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urFG5s4LfGdJ",
        "outputId": "cbdbb16f-2c4e-4ab2-aab3-6125e536618b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words: ['Independence', 'Day', 'is', 'one', 'of', 'the', 'important', 'festivals', 'for', 'every', 'Indian', 'citizen', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4. Stemming**\n",
        "\n",
        "stemming is used to normalize words into its base form or root form. For example, celebrates, celebrated and celebrating, all these words are originated with a single root word \"celebrate.\" The big problem with stemming is that sometimes it produces the root word which may not have any meaning.\n",
        "\n",
        "For Example, intelligence, intelligent, and intelligently, all these words are originated with a single root word \"intelligen.\" In English, the word \"intelligen\" do not have any meaning.\n",
        "\n",
        "**Code Example:**"
      ],
      "metadata": {
        "id": "CfoA2YwSbodT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Apply stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(\"Stemmed Words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clHKAR2fWzZV",
        "outputId": "4572bba1-c1d6-48b7-a5a8-72f8978d5da5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Words: ['independ', 'day', 'is', 'one', 'of', 'the', 'import', 'festiv', 'for', 'everi', 'indian', 'citizen', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **5. Lemmatization**\n",
        "\n",
        "Lemmatization is quite similar to the Stamming. It is used to group different inflected forms of the word, called Lemma. The main difference between Stemming and lemmatization is that it produces the root word, which has a meaning.\n",
        "\n",
        "For example: In lemmatization, the words intelligence, intelligent, and intelligently has a root word intelligent, which has a meaning.\n",
        "\n",
        "**Code Example:**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6v-CyNovirdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Apply lemmatization\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(\"Lemmatized Words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1RJxCq1WzcH",
        "outputId": "31a47adc-28aa-4827-fb18-b4f13ab7a771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Words: ['Independence', 'Day', 'is', 'one', 'of', 'the', 'important', 'festival', 'for', 'every', 'Indian', 'citizen', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **6. Identifying Stop Words**\n",
        "\n",
        "In English, there are a lot of words that appear very frequently like \"is\", \"and\", \"the\", and \"a\". NLP pipelines will flag these words as stop words. Stop words might be filtered out before doing any statistical analysis.\n",
        "\n",
        "Example: He is a good boy.\n",
        "\n",
        "**Code Example:**"
      ],
      "metadata": {
        "id": "hezWGznvjKQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Filter out stop words\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "print(\"Filtered Words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKhPo3NcWze3",
        "outputId": "53bdeeb6-1f15-404c-e895-efa32de25ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Words: ['Independence', 'Day', 'one', 'important', 'festivals', 'every', 'Indian', 'citizen', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **7. Dependency Parsing**\n",
        "\n",
        "Dependency Parsing is used to find that how all the words in the sentence are related to each other.\n",
        "\n",
        "**Code Example Using spaCy:**"
      ],
      "metadata": {
        "id": "-lF1VIkEjp0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Parse the sentence\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Print dependency relations\n",
        "for token in doc:\n",
        "    print(f'{token.text:12} {token.dep_:10} {token.head.text}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbvWcSGjjqBn",
        "outputId": "0ca4e888-6eee-4e10-bb7b-23d0fae49a74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Independence compound   Day\n",
            "Day          nsubj      is\n",
            "is           ROOT       is\n",
            "one          attr       is\n",
            "of           prep       one\n",
            "the          det        festivals\n",
            "important    amod       festivals\n",
            "festivals    pobj       of\n",
            "for          prep       festivals\n",
            "every        det        citizen\n",
            "Indian       amod       citizen\n",
            "citizen      pobj       for\n",
            ".            punct      is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **8. POS Tagging**\n",
        "\n",
        "POS stands for parts of speech, which includes Noun, verb, adverb, and Adjective. It indicates that how a word functions with its meaning as well as grammatically within the sentences. A word has one or more parts of speech based on the context in which it is used.\n",
        "\n",
        "Example: \"Google\" something on the Internet.\n",
        "\n",
        "In the above example, Google is used as a verb, although it is a proper noun.\n",
        "\n",
        "**Code Example:**"
      ],
      "metadata": {
        "id": "E1quWxFkkdL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag, ne_chunk\n",
        "\n",
        "# Download the 'averaged_perceptron_tagger' resource\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# POS tagging\n",
        "pos_tags = pos_tag(words)\n",
        "print(\"POS Tags:\", pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NnJ2CnfjkDSb",
        "outputId": "33541765-f24a-45d4-df90-2a0a153c446f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: [('Independence', 'NNP'), ('Day', 'NNP'), ('is', 'VBZ'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('important', 'JJ'), ('festivals', 'NNS'), ('for', 'IN'), ('every', 'DT'), ('Indian', 'JJ'), ('citizen', 'NN'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **9. Named Entity Recognition (NER)**\n",
        "\n",
        "Named Entity Recognition (NER) is the process of detecting the named entity such as person name, movie name, organization name, or location.\n",
        "\n",
        "Example: Steve Jobs introduced iPhone at the Macworld Conference in San Francisco, California.\n",
        "\n",
        "**Code Example Using spaCy:**"
      ],
      "metadata": {
        "id": "OpvV7SVkksUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Named Entity Recognition\n",
        "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "print(\"Named Entities:\", entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOKdTvJ9ksdF",
        "outputId": "e32b5d10-3a1a-460b-a39c-5442582f8bb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities: [('Independence Day', 'EVENT'), ('Indian', 'NORP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **10. Chunking**\n",
        "\n",
        "Chunking is used to collect the individual piece of information and grouping them into bigger pieces of sentences.\n",
        "\n",
        "**Code Example:**"
      ],
      "metadata": {
        "id": "EyaF6tb1kvqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the 'words' resource\n",
        "nltk.download('words')\n",
        "# Chunking using NLTK\n",
        "def get_chunks(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged = pos_tag(tokens)\n",
        "    chunks = ne_chunk(tagged)\n",
        "    return chunks\n",
        "\n",
        "chunks = get_chunks(sentence)\n",
        "print(\"Chunks:\", chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIWt78BOkvzu",
        "outputId": "9a633d22-ca94-4f34-9044-74b0d2952dab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunks: (S\n",
            "  (GPE Independence/NNP)\n",
            "  Day/NNP\n",
            "  is/VBZ\n",
            "  one/CD\n",
            "  of/IN\n",
            "  the/DT\n",
            "  important/JJ\n",
            "  festivals/NNS\n",
            "  for/IN\n",
            "  every/DT\n",
            "  (GPE Indian/JJ)\n",
            "  citizen/NN\n",
            "  ./.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Tokenization**:\n",
        "   - `sent_tokenize` splits the text into sentences.\n",
        "   - `word_tokenize` splits the text into words.\n",
        "\n",
        "2. **Lowercasing**:\n",
        "   - Converts all words to lowercase to ensure uniformity.\n",
        "\n",
        "3. **Stopword Removal**:\n",
        "   - Removes common words (e.g., \"and\", \"the\") that do not contribute significant meaning.\n",
        "\n",
        "4. **Punctuation Removal**:\n",
        "   - Removes punctuation using regular expressions.\n",
        "\n",
        "5. **Stemming**:\n",
        "   - Reduces words to their root form using the `PorterStemmer`.\n",
        "\n",
        "6. **Lemmatization**:\n",
        "   - Reduces words to their base or dictionary form using the `WordNetLemmatizer`.\n",
        "\n",
        "7. **Text Normalization**:\n",
        "   - Combines several normalization steps, including lowercasing, removing punctuation, digits, and extra spaces."
      ],
      "metadata": {
        "id": "xqxiGpu92pZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download required NLTK data files (only need to do this once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Sample text\n",
        "text = \"Hello! This is an example sentence. The quick brown foxes are jumping over the lazy dogs.\"\n",
        "\n",
        "# 1. Tokenization\n",
        "def tokenize_text(text):\n",
        "    sentences = sent_tokenize(text)  # Tokenize into sentences\n",
        "    words = word_tokenize(text)       # Tokenize into words\n",
        "    return sentences, words\n",
        "\n",
        "sentences, words = tokenize_text(text)\n",
        "print(\"Sentences:\", sentences)\n",
        "print(\"Words:\\n\", words)\n",
        "\n",
        "# 2. Lowercasing\n",
        "def lowercase_text(words):\n",
        "    return [word.lower() for word in words]\n",
        "\n",
        "lowercase_words = lowercase_text(words)\n",
        "print(\"Lowercase Words:\\n\", lowercase_words)\n",
        "\n",
        "# 3. Stopword Removal\n",
        "def remove_stopwords(words):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [word for word in words if word not in stop_words]\n",
        "\n",
        "words_no_stopwords = remove_stopwords(lowercase_words)\n",
        "print(\"Words without Stopwords:\\n\", words_no_stopwords)\n",
        "\n",
        "# 4. Punctuation Removal\n",
        "def remove_punctuation(words):\n",
        "    return [re.sub(r'[^\\w\\s]', '', word) for word in words]\n",
        "\n",
        "words_no_punctuation = remove_punctuation(words_no_stopwords)\n",
        "print(\"Words without Punctuation:\\n\", words_no_punctuation)\n",
        "\n",
        "# 5. Stemming\n",
        "def stem_words(words):\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(word) for word in words]\n",
        "\n",
        "stemmed_words = stem_words(words_no_punctuation)\n",
        "print(\"Stemmed Words:\\n\", stemmed_words)\n",
        "\n",
        "# 6. Lemmatization\n",
        "def lemmatize_words(words):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "lemmatized_words = lemmatize_words(words_no_punctuation)\n",
        "print(\"Lemmatized Words:\\n\", lemmatized_words)\n",
        "\n",
        "# 7. Text Normalization\n",
        "def normalize_text(text):\n",
        "    text = text.lower()  # Lowercasing\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.strip()  # Remove leading and trailing spaces\n",
        "    return text\n",
        "\n",
        "normalized_text = normalize_text(text)\n",
        "print(\"Normalized Text:\\n\", normalized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nzph9jGo2fdL",
        "outputId": "8cdaf676-2b24-47ff-9864-e9d9ca906e5d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['Hello!', 'This is an example sentence.', 'The quick brown foxes are jumping over the lazy dogs.']\n",
            "Words:\n",
            " ['Hello', '!', 'This', 'is', 'an', 'example', 'sentence', '.', 'The', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy', 'dogs', '.']\n",
            "Lowercase Words:\n",
            " ['hello', '!', 'this', 'is', 'an', 'example', 'sentence', '.', 'the', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy', 'dogs', '.']\n",
            "Words without Stopwords:\n",
            " ['hello', '!', 'example', 'sentence', '.', 'quick', 'brown', 'foxes', 'jumping', 'lazy', 'dogs', '.']\n",
            "Words without Punctuation:\n",
            " ['hello', '', 'example', 'sentence', '', 'quick', 'brown', 'foxes', 'jumping', 'lazy', 'dogs', '']\n",
            "Stemmed Words:\n",
            " ['hello', '', 'exampl', 'sentenc', '', 'quick', 'brown', 'fox', 'jump', 'lazi', 'dog', '']\n",
            "Lemmatized Words:\n",
            " ['hello', '', 'example', 'sentence', '', 'quick', 'brown', 'fox', 'jumping', 'lazy', 'dog', '']\n",
            "Normalized Text:\n",
            " hello this is an example sentence the quick brown foxes are jumping over the lazy dogs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation:\n",
        "\n",
        "1. **Bag of Words (BoW):**\n",
        "   - The `CountVectorizer` is used to convert the text into a matrix of token counts.\n",
        "   - The resulting matrix is used for training a Naive Bayes classifier.\n",
        "\n",
        "2. **TF-IDF:**\n",
        "   - The `TfidfVectorizer` transforms the text into TF-IDF features.\n",
        "   - The resulting matrix is used similarly to train and evaluate a classifier.\n",
        "\n",
        "3. **Word Embeddings (Word2Vec):**\n",
        "   - `Word2Vec` is trained using `gensim` to create dense vector representations of words.\n",
        "   - For each document, we compute the average of the word vectors to get a document vector.\n",
        "   - This representation is used to train and evaluate the classifier.\n",
        "\n",
        "4. **Text Classification:**\n",
        "   - The `MultinomialNB` classifier is used for simplicity.\n",
        "   - We evaluate the classifier’s performance using accuracy and other metrics with `classification_report`.\n"
      ],
      "metadata": {
        "id": "aQCS9RQmDgEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Download required NLTK data files (only need to do this once)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample text data and labels\n",
        "texts = [\n",
        "    \"I love programming and solving problems.\",\n",
        "    \"The weather is so nice today.\",\n",
        "    \"I am excited about the new movie release.\",\n",
        "    \"This programming course is very challenging.\",\n",
        "    \"I enjoy taking long walks in the park.\",\n",
        "    \"The movie had a great plot and amazing visuals.\",\n",
        "    \"I dislike doing repetitive tasks.\",\n",
        "    \"The weather forecast is not accurate.\",\n",
        "    \"Programming can be very rewarding.\",\n",
        "    \"I love watching science fiction movies.\"\n",
        "]\n",
        "labels = [\"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"positive\", \"negative\", \"negative\", \"positive\", \"positive\"]\n",
        "\n",
        "# 1. Bag of Words (BoW)\n",
        "def bag_of_words(texts):\n",
        "    vectorizer = CountVectorizer()\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    return X, vectorizer\n",
        "\n",
        "# 2. Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "def tfidf_representation(texts):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    return X, vectorizer\n",
        "\n",
        "# 3. Word Embeddings (Word2Vec)\n",
        "def train_word2vec(texts):\n",
        "    tokenized_texts = [simple_preprocess(text) for text in texts]\n",
        "    model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
        "    return model\n",
        "\n",
        "def get_average_word2vec(texts, model):\n",
        "    def average_word2vec(text):\n",
        "        words = simple_preprocess(text)\n",
        "        word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "        if len(word_vectors) == 0:\n",
        "            return np.zeros(model.vector_size)\n",
        "        return np.mean(word_vectors, axis=0)\n",
        "\n",
        "    return np.array([average_word2vec(text) for text in texts])\n",
        "\n",
        "# Example: Classification using different representations\n",
        "def train_and_evaluate_classifier(X_train, y_train, X_test, y_test):\n",
        "    clf = MultinomialNB()\n",
        "    clf.fit(X_train, y_train)\n",
        "    predictions = clf.predict(X_test)\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "\n",
        "# Preprocessing data\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Bag of Words\n",
        "X_bow, bow_vectorizer = bag_of_words(texts)\n",
        "print(\"Bag of Words - Shape:\", X_bow.shape)\n",
        "\n",
        "# TF-IDF\n",
        "X_tfidf, tfidf_vectorizer = tfidf_representation(texts)\n",
        "print(\"TF-IDF - Shape:\", X_tfidf.shape)\n",
        "\n",
        "# Word Embeddings\n",
        "word2vec_model = train_word2vec(texts)\n",
        "X_w2v = get_average_word2vec(texts, word2vec_model)\n",
        "print(\"Word2Vec - Shape:\", X_w2v.shape)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X_bow, encoded_labels, test_size=0.3, random_state=42)\n",
        "X_train_tfidf, X_test_tfidf, _, _ = train_test_split(X_tfidf, encoded_labels, test_size=0.3, random_state=42)\n",
        "X_train_w2v, X_test_w2v, _, _ = train_test_split(X_w2v, encoded_labels, test_size=0.3, random_state=42)\n",
        "\n",
        "# Evaluate classifiers for each representation\n",
        "print(\"\\nEvaluating Bag of Words:\")\n",
        "train_and_evaluate_classifier(X_train_bow, y_train, X_test_bow, y_test)\n",
        "\n",
        "print(\"\\nEvaluating TF-IDF:\")\n",
        "train_and_evaluate_classifier(X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
        "\n",
        "print(\"\\nEvaluating Word2Vec:\")\n",
        "train_and_evaluate_classifier(X_train_w2v, y_train, X_test_w2v, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6F7WMzjfDaXH",
        "outputId": "f2d946ba-30d5-4595-82f1-9dab534408c5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words - Shape: (10, 46)\n",
            "TF-IDF - Shape: (10, 46)\n",
            "Word2Vec - Shape: (10, 100)\n",
            "\n",
            "Evaluating Bag of Words:\n",
            "Classification Report:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         0\n",
            "           1       1.00      0.33      0.50         3\n",
            "\n",
            "    accuracy                           0.33         3\n",
            "   macro avg       0.50      0.17      0.25         3\n",
            "weighted avg       1.00      0.33      0.50         3\n",
            "\n",
            "\n",
            "Evaluating TF-IDF:\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         0\n",
            "           1       1.00      0.67      0.80         3\n",
            "\n",
            "    accuracy                           0.67         3\n",
            "   macro avg       0.50      0.33      0.40         3\n",
            "weighted avg       1.00      0.67      0.80         3\n",
            "\n",
            "\n",
            "Evaluating Word2Vec:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Negative values in data passed to MultinomialNB (input X)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d954e49af52c>\u001b[0m in \u001b[0;36m<cell line: 96>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEvaluating Word2Vec:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m \u001b[0mtrain_and_evaluate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_w2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_w2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-d954e49af52c>\u001b[0m in \u001b[0;36mtrain_and_evaluate_classifier\u001b[0;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_and_evaluate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Classification Report:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 )\n\u001b[1;32m   1151\u001b[0m             ):\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_counters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mcheck_non_negative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MultinomialNB (input X)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_non_negative\u001b[0;34m(X, whom)\u001b[0m\n\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mX_min\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1489\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Negative values in data passed to %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mwhom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uACVbmyyDroM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}